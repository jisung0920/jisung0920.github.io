## Singular Value Decomposition

M(mxn) 행렬이 있고, M의 랭크를 r로 둡시다.   
그러면 U, V, $\Sigma$ 를 다음과 같이 찾을 수 있습니다.  

<img src="11.Dimensionality Reduction.assets/image-20200725094316816.png" alt="image-20200725094316816" style="zoom:50%;" />

1. U 는 $m\times r$ 의 column-orthonomal matrix 입니다.   
   이것의 칼럼들은 각각 유닛벡터입니다. (각 칼럼끼리의 dot 이 0)
2. V 는 $n\times r$ 의 column-orthonormal matrix 입니다.  
   여기서 V 는 transpose 형태로 사용합니다.   
   그래서 $V^T$ 의 행은 orthonormal 입니다.
3. $\Sigma$ 는 diagonal matrix 입니다.  
   대각 성분 외의 모든 요소는 0 입니다.   
   이 행렬의 성분들은 M의 singular value 라고 합니다.



그림 11.6은 movie-user 레이팅에 관한 2랭크 행렬을 보여줍니다. 

<img src="11.Dimensionality Reduction.assets/image-20200725094546432.png" alt="image-20200725094546432" style="zoom:33%;" />

여기서 영화에 2개의 conncept 이 숨겨져 있습니다.  
매트릭스, 에일리언, 스타워즈는 SF 영화이고  
카사블랑카와 타이타닉은 로맨스 영화입니다.    
이 정보는 명시적으로 파악할 수 없습니다. 

모든 남자들(Joe, Jim, John, Jack)은 SF 에 평가를 했고,  
여자(Jill, Jenny, Jane)들은 로맨스에만 평가를 했습니다.   
이 행렬에서는 2개의 rank 를 갖습니다.

이 행렬 M의 decomposition 은 그림 11.7 과 같습니다.  
 (SVD decomposition 은 추후 선형대수 포스팅에서 다룰 예정입니다.)

<img src="11.Dimensionality Reduction.assets/image-20200725100043966.png" alt="image-20200725100043966" style="zoom:50%;" />



SVD 을 사용하는 목적은 기존 행렬에 숨겨진 표상(represent)된 concept들을 찾는 것 입니다.   
위의 예시에서, 숨겨진 컨셉으로 SF와 로맨스가 있었습니다.  
@@M의 행들을 사람으로 보고 열들을 영화로 봅시다.  
그러면 행렬 U 는 사람과 컨셉을 연결합니다.  
예를들어 M의 1 행은 Joe 이고 SF 컨셉만 좋아합니다.  
U 의 1행 1열의 값 0.14 는 칼럼의 다른 엔트리보다 작습니다.  
이는 joe 가 그 영화들을 다른 사람에 비해 점수를 낮게 주었기 때문입니다.    
두번째 칼럼의 경우 0 인데 이는 joe 가 로맨스 영화에 대한 평가를 하나도 안했기 때문입니다.

행렬은 V 영화-개념과 관련있습니다.   
$V^T$ 의 첫번째 부터 세번째 칼럼의 첫 행의 값 0.58 은 3개의 영화 매트릭스, 에일리언, 스타워즈를 가리킵니다.  
이것들은 SF 장르이고, 나머지 두 칼럼에서 0 은 이 영화들이 로맨스 개념에 대한 성질이 없다는 것을 말합니다.   

마지막으로, S 행렬은 각 개념들에 대한 강도를 나타냅니다.  
예시에서, SF 컨셉의 강도는 12.4 이고 로맨스의 강도는 9.5 입니다.  
직관적으로, SF 컨셉이 더 강합니다.  
데이터는 SF의 명화와 그 것을 좋아하는 사람들에 대한 정보를 더 많이 제공하고 있기 때문입니다.

일반적으로, 컨셉들은 명확하게 묘사되진 않습니다.  
S 가 항상 대각행렬이고 대각 외의 것들이 항상 0이 더라도, U와 V에 더 적은 0 이 있을 것입니다.  
M의 행과 열로 표현된 엔티티들은 더 다양한 컨셉들의 성질이 있을 것 입니다.  
사실, 11.8  의 decomposition 은 특히 단순합니다. 실제에서는 이렇게 단순하지 않습ㄴ디ㅏ.  
M의 랭크가 USV 행렬에 대해 원하는 칼럼의 수보다 더 크면, 그 분해는 정확해지지 않습니다.  
최적의 근사치를 얻기 위해, 우리는 정확한 분해로 부터 가장 작은 singular value 를 따르는 칼럼을 제거하는 것이 필요합니다.   
다음 예시는 11.8을 약간 수정한 것입니다.

<img src="11.Dimensionality Reduction.assets/image-20200725104219769.png" alt="image-20200725104219769" style="zoom:50%;" />

#### example 11.9

<img src="11.Dimensionality Reduction.assets/image-20200725105409551.png" alt="image-20200725105409551" style="zoom:50%;" />

Jill 과 Jane 이 에일리언에 점수를 매겼습니다.  
이전에는 rank 2 였지만 이제 3이되었습니다.  
S 에서 첫번째는 SF, 두번째는 로맨스이고  
3번째의 컨셉은 설명하긴 어렵습니다. 그러나 이것은 그렇게 중요하진 않습니다. weight 가 작습니다.

다음 세션에선 덜 중요한 개념을 제거하는 법에 대해 다룹니다.

### 11.3.3 Dimensionality Reduction Using SVD 

엄청 큰 행렬 M 을 USV 로 표현하는 작업을 가정해 봅시다.  
그러나 행렬이 너무커서 쉽게 저장할 수 없습니다.  
이 세개의 행렬을 차원적으로 줄이는 최고 의 방법은 가장 작은 특이값을 0으로 바꾸는 것입니다.  
s 의 작은 특이값을 0으로 바꾸면 이에 따르는 U, V의 s 칼럼들을 제거할 수 있습니다.

#### example 11.10 

11.9 예시는 3개의 특이값을 갖었습니다. 여기서 2개의 차원으로 줄이고 싶습니다.   
그러면 우리는 가장작은 특이값 1.3 을 0 으로 바꿉니다.   
U의 3번째 칼럼과 $V^T$ 의 3번째 행은 연산시 0만 곱해지므로, 이 행과 칼럼은 없어도 됩니다.  
M' 에 근사치는 두개의 특이값만 사용해서 얻을 수 있습니다. 

<img src="11.Dimensionality Reduction.assets/image-20200725111146983.png" alt="image-20200725111146983" style="zoom:50%;" />

이상적으로 전체 차이는 마지막 특이값을 0으로 만든 결과입니다.   
그러나 이는 단순한 예시이고, 더 차이가 있습니다. 그 차이의 대부분은 M '의 분해가 두 개의 유효 자릿수로만 정확했기 때문에 발생하는 반올림 오차로 인한 것입니다.



### 11.3.4 Why Zeroing Low Singular Values Works

작은 특이값을 버리는 선택은 M과의 rmse 를 최소화하는 것을 보여줄 수 있습니다.  
엔티티의 수는 고정되어 있고 스퀘어루트는 단조로운 연산(monotone)이기 때문에, 우리는 단순화 할 수 있고 행렬과 관련된 Frobenius norm 을 비교할 수 있습니다.  
M 이 그것의 근사치와 차이가 있다면 ||M|| 은 그 행렬들 사이의 rmse 에 비례합니다.

왜 가장 작은 특이값을 0 으로 만드는 것으로 rmse 를 최소화 하는지 설명하기 위해서, 작은 행렬 대수부터 시작합시다.  
M 이 3개의 행렬의 곱(product)이라고 가정해 봅시다. M = PQR, $m_{ij},p_{ij},q_{ij},r_{ij}$ 는 각 행렬의 성분입니다.  
그러면 행렬 곱의 정의에 따라 다음이 나옵니다.  

<img src="11.Dimensionality Reduction.assets/image-20200725112854558.png" alt="image-20200725112854558" style="zoom:50%;" />

그러면

<img src="11.Dimensionality Reduction.assets/image-20200725112920090.png" alt="image-20200725112920090" style="zoom:50%;" />

우리가 텀의 합의 제곱을 할 때, 식 11.1 의 오른쪽 처럼,  우리는 효율적으로 합의 두개의 카피들을 만들 수 있고 첫번째 합의 각 텀을 두번째 합의 각 텀에 곱할 수 있습니다.

<img src="11.Dimensionality Reduction.assets/image-20200725113231418.png" alt="image-20200725113231418" style="zoom:50%;" />

<img src="11.Dimensionality Reduction.assets/image-20200725113556306.png" alt="image-20200725113556306" style="zoom:50%;" />

이제 PQR을 $U\Sigma V^T$ 라고 합시다.   
R은 row-orthonomal 이고, 그것의 행들은 유닛벡터입니다, 그리고 행의 dot product 는 0이 됩니다.  
Q가 대각행렬이기 때문에 k=l, n=m 일때 빼고는 $q_{kl} ,q_{nm}$  은 0이 됩니다.  그래서 l 과 m 에대한 서메이션은 빼도 됩니다.  그래서 다음과 같이 됩니다.

<img src="11.Dimensionality Reduction.assets/image-20200725114525029.png" alt="image-20200725114525029" style="zoom:50%;" />

다음 서메이션을 재순서합니다. 그래서 i 가 내부에 있도록 합니다.  
11.3 은 $p_{ik} p_{in}$ 만 i 와 관련이 있습니다.  모든 다른 항들은 상수로 보고 i 에 관한 서메이션만 신경습니다.  
P 가 column-orthonormal 이기 때문에, k 가 n 일때  $\Sigma_ip_{ik}p_{in} = 1$ 이고 그외에는 0 입니다.  
그러므로 k=n 으로 설정하고  $p_{ik} p_{in}$ 항을 버릴 수 있습니다. 다음과 같이 줄일 수 있습니다.

<img src="11.Dimensionality Reduction.assets/image-20200725115122720.png" alt="image-20200725115122720" style="zoom:50%;" />

R은 row-orthonormal 이므로 $\Sigma_jr_{kj}r_{kj} = 1$ 입니다. 그래서 $r_{kj}$ 항과 j 에 대한 합을 제거할 수 있습니다.  
그래서 다음과 같이 단순한 식을 만들 수 있습니다.  

<img src="11.Dimensionality Reduction.assets/image-20200725115436133.png" alt="image-20200725115436133" style="zoom:50%;" />

M - M' = $U(S - S')V^T$ 이므로   
$||M - M'||^2$ 를 S-S' 의 대각 요소의 제곱합과 동일하게 볼 수 있습니다.  
이를 최소화 하기 위해서는 특이값에서 가장 작은 요소를 선택해야합니다.

#### How Many Singular Values Should We Retain?

유용한 rule of thumb(경험법칙) 은 S의 energy 의 90%으로 특이값을 유지하는 것입니다.  
이것은 유지한 특이값의 제곱합이 적어도 전체 특이값의 제곱합의 90%는 되어야 한다는 의미입니다.  
11.10 예시에서는 $12.4^2 + 9.5^2 + 1.3^2 = 245.70$ 이 전체 energy이고, 유지한 energy 는 $12.4^2  + 9.5^2 = 244.01$ 으로 99% 입니다.

### 11.3.5 Querying Using Concepts

이 세션에서 우리는 어떻게 SVD 가 특정 쿼리에 대해 효율적이고 좋은 정확도로 답을 하는지에 대해 살핍니다.   
우리는 그림11.7 의 SVD 형태로 구성된 기존 영화 평가 데이터를 분해 한 것을 갖는다고 가정해봅시다.

A라는 사람이 매트릭스를 보고 4점을 메겼다고 합시다. 그러면 이 사람의 벡터는 q=[4,0,0,0,0] 입니다.  

만약 CF 방식을 사용한다면, 우리는 A 를 기존 행렬에 있는 다른 사람들과 비교를 합니다.   
대신, 우리는 A 를 행렬 V 에 곱하여 concept space 에 매핑할 수 있습니다.  
우리는 qV = [2.32, 0] 이라는 것을 찾을 수 있습니다.  
이것은 A 가 SF 에 관심이 있다는 것을 의미합니다.  

우리는 이제 컨셉 공간에서 A를 표현했지만 원래의 movie space에서 파생 된 표현과는 다릅니다.  
우리가 그의 표상을 영화 공간에 매핑하는 유용한 방법은 [2.32,0] 을 $V^T$ 와 곱하는 것입니다.   
이는 [1.35,1.35,1.35,0,0] 입니다. 이는 A 에게 에일리언, 스타워즈를 제시합니다.

우리가 개념 공간에서 수행할 수 잇는 다른 쿼리의 종류는, A 와 유사한 사용자를 찾아주는 것입니다.  
우리는 V 를 컨셉공간의 모든 사용자에게 매핑하는데 사용할 수 있습니다.  
예를들어, Joe 는 [1.74,0] 으로 Jill 은 [0, 5.68] 으로 맵이 됩니다.  
이렇게 컨셉 공간의 사용자 정보는 코사인 거리를 사용하여 사용자 간 유사도를 측정할 수 있습니다.  
예시에서는 A 가 Joe 와 유사합니다.(0) Jill 과는 코사인 거리가 1 이됩니다.   