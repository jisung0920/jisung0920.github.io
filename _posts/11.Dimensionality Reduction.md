## 11. Dimensionality Reduction

데이터를 큰 행렬로 볼 수 있는 많은 소스가 존재합니다.   
Chap 5 에 transition matrix, 10에서 소셜 네트워크 등..  
그 행렬들은 기존과 유사한 'narrower' 행렬을 찾아서 요약할 수 있습니다.   
이 'narrower' 행렬은  행이나 열이 더 작아서 더 효율적으로 사용할 수 있습니다.

이 단원에서는 차원 축소의 아이디어를 좀 더 자세하게 다룹니다.  
PCA 에서 고유값에 대한 사용에 대해 다루고,  
UVD의 더 강력한 버전인 SVD 에 대해 다룹니다.  
마지막에는 대규모 데이터에서 사용하는 CUR 이라는 다른 형태의 분해 방식을 다룹니다. 이는 SVD의 변종입니다. (sparse 를 유지하면서 분해하는 방법)



## 11.1 Eigenvalues and Eigenvectors of Symmetric Matrices

### 11.1.1 Definitions

L 을 정방 향렬, $\lambda$ 를 상수, e 를 행에서 동일한 숫자를 갇는 nonzero 열 벡터라 합시다.    
$Me = \lambda e$  라면, $\lambda$ 를 M의 고유값, e 를 고유벡터라고 합니다. 

만약 e 가 고유벡터이고 c 가 아무 상수이면, ce 도 동일한 고유값을 갖는 고유벡터입니다.  
상수를 곱하는 것은 벡터의 길이는 바꾸지만 방향을 바꾸지는 않습니다.  
그래서 길이에 따라 애매한 것을 피하기 위해, 모든 고유벡터를 유닛 벡터로 해서 다룹니다. (성분의 제곱합이 1인 것 )  
이것은 고유벡터를 유일하게 만들기엔 조금 부족합니다, -1 을 곱한다면, 제곱합이여도 반대 방향의 벡터가 나와서 유일하지 않게됩니다.  
그래서 일반적으로 고유벡터의 첫번째 nonzero 성분이 양수가 되도록 요구합니다.

### 11.1.2 Computing Eigenvalues and Eigenvectors

우리는 이미 고유쌍을 찾는 방식에 대해 보았습니다. $M^iv$ 를 수렴할 때 까지 반복하는 것으로  
M 이 확률벡터일 떄, limiting 벡터는 principal 고유벡터(제일 큰 고유값을 갖는 벡터)이고 이는 고유값 1을 갖었습니다.  
이런식으로 고유벡터를 찾는 것은 (power iteration) 꽤 잘 작동합니다. principal 고유값이 1이 아니더라도, 그러고 i 가 커짐어 때라, $M^{i+1}v$ 와 $M^{i}v$  의 비율이 principal 고유값에 가까워집니다, 그 동안 $M^iv$ 는  principal 고유벡터와 같은 방향인 한 벡터에 가까워집니다.

우리는 모든 고유쌍을 찾기위한 power-iteration 방식의 일반화를 다룹니다.  
그러나 모든 고유쌍을 계산하는 $O(n^3)$ 의 소요되는 방식이 있고 이 방식을 먼저 소개드립니다.   
고유값이 동일한 경우이여도 항상 n 개의 고유쌍이 존재할 것입니다.  
이 방식은 고유쌍에 대한 정의를 다루는 것부터 시작합니다.  
$Me= \lambda e $ 를 $(M - \lambda I)e = 0$

선형대수에 따라, $(M - \lambda I)e = 0$  일 때 $e\neq 0$ 이라면, $M - \lambda I$ 의 디터미넌트는 0 이되어야 합니다.   
M 이 대각 성분으로 c 를 갖는 다면 디터미넌트에 $c-\lambda$ 이 있을 것입니다.  
nn 행렬의 디터미넌트는 n! 의 텀을 갖지만, 그것은 O(n^3) 시간으로 풀수 있는 방식들이 있습니다.   
하나의 예시로는 pivotal condensation 입니다.

$M - \lambda I$ 의 디터미넌트는 n차의 폴리노미알입니다. 그래서 우리는 고유값인 람다의 n 개의 값을 얻을 수 있습니다.  
어떠한 값 c 에 대해 우리는 Me = ce 를 풀 수 있습니다.   
n 개의 미지수에 n 개의 식이 있으나, 어떠한 식에도 상수는 없습니다.  우리는 상수 항 안에서 e 에 대해 풀 수 있습니다.  
그러나 우리는 성분의 제곱합을 1로 만들어 정규화 해서 식을 얻고 고유값 c 에 따른 고유벡터를 얻을 수 있습니다.



### 11.1.3 FInding Eigenpairs by Power Iteration

우리는 이제 5.1.에서 사용했던 방식의 일반화를 확인합니다.(페이지랭크에서 찾은 것)  
우리는 약간 일반화된 방식으로 pricipal 고유벡터를 계산하는 것부터 시작합니다. 그리고 pricipal 고유벡터를 제거하기 위해 행렬을 수정합니다.  
주 고유 벡터가 실제 행렬의 두번째 고유벡터인 새로운 행렬이 결과가 됩니다.  
이 방식은 다음과 같습니다.  
찾은 고유벡터를 제거하고, power iteration 를 사용해서 남아 있는 행렬의 주 고유벡터를 찾습니다. 

어떠한 nonzero 벡터 $x_0$ 부터 시작해서 다음을 반복합니다.  

$x_{k+1} := \frac{Mx_k}{||Mx_k||}$

$||N||$ 은 Frobenius norm 입니다. 이것은 N 의 성분들의 제곱합의 루트를 씌운 것입니다.   
현재 벡터 x_k 를 M 에 수렴할때까지 곱합니다.  $||x_k -x_{k+1}||$ 이 임계값 보다 작을 때 까지  
x 를 수렴된 x_k 라고 합시다. 그러면 x 는 거의 주 고유벡터가 됩니다.  
그에 따른 고유값을 얻기 위해서는 $\lambda = x^T M x $ 로 계산합니다.

#### example 11.3  

x_0  을 1인 벡터로 두고 시작합니다.  
<img src="11.Dimensionality Reduction.assets/image-20200724153648226.png" alt="image-20200724153648226" style="zoom:50%;" />

여기서 x_1 은 결과 5, 8의 Frobenius norm 으로 둡니다. $\sqrt{5^2 + 8^2} = \sqrt{89}=9.434$  로 나눕니다.

<img src="11.Dimensionality Reduction.assets/image-20200724153818307.png" alt="image-20200724153818307" style="zoom:50%;" />

이 값에 M 을 곱하면

<img src="11.Dimensionality Reduction.assets/image-20200724153838169.png" alt="image-20200724153838169" style="zoom:50%;" />

여기서 Fro norm 은 6.971 이 나오고

<img src="11.Dimensionality Reduction.assets/image-20200724153910064.png" alt="image-20200724153910064" style="zoom:50%;" />

이렇게 반복하다 보면 주 고유벡터

<img src="11.Dimensionality Reduction.assets/image-20200724153935052.png" alt="image-20200724153935052" style="zoom:50%;" />

 가 나오고   
주 고유값는 다음과 같습니다.

<img src="11.Dimensionality Reduction.assets/image-20200724154027229.png" alt="image-20200724154027229" style="zoom:50%;" />

 이전 예시에서 주 고유값이 7 나왔는데 유사합니다. (정확한 값이 나오기 전에 멈춰서 오차가 있습니다.)  
약간의 부정확성은 크게 중요치 않습니다. 

두번째 고유쌍을 찾기 위해 우리는 새로운 행렬 $M^* = M -\lambda_1 xx^T$ 를 만듭니다.  
그러고 power iteration 을 사용해서 M\*의 최대 고유값을 계산합니다.   
얻은 x\* 와 $\lambda^*$ 는 M 의 두번째로 가장 큰 고유값과 고유벡터 입니다. 

직관적으로, 우리가 한 것은 관련된 고유 값을 0으로 설정하여 주어진 고유 벡터의 영향을 제거하는 것입니다.  
포멀한 정의는 두개의 옵저베이션을 따릅니다.  
만약 $M^* = M - \lambda x x^T $ 라면, 

1. x 또한 M\* 의 고유벡터입니다. 그리고 그것의 고유값은 0 입니다.  
   이를 증명하자면  
     <img src="11.Dimensionality Reduction.assets/image-20200724155031531.png" alt="image-20200724155031531" style="zoom:50%;" />    
   여기서 x가 유닛벡터 이므로  $x^T x  = 1$ 입니다.
2. 반대로, 만약에 v 와 $\lambda_v$ 가 대칭 행렬 M 의 고유쌍이고, (x, $\lambda$) 보다 먼저나온다면, 그것도 M\* 의 고유쌍입니다.  
   <img src="11.Dimensionality Reduction.assets/image-20200724155348158.png" alt="image-20200724155348158" style="zoom:50%;" />  
   위 식은 다음을 사용합니다.  
   1. M 은 대칭행렬이므로 M =M^T
   2. 대칭행렬의 고유벡터는 orthgonal 입니다. (dot product 는 0 이됩니다.)

#### example 11.4

<img src="11.Dimensionality Reduction.assets/image-20200724155607515.png" alt="image-20200724155607515" style="zoom:50%;" />

M\* 은 위와 같이 구하고,  
동일하게 반복해서 고유쌍을 구할 수 있습니다.

### 11.1.4 The Matrix of Eigenvectors

우리가 대칭행렬 M 이 있고, 그것의 고유벡터들을 $e_1,e_2,\dots, e_n$ 인 열백터로 봅시다.  
E를 고유벡터들을 열벡터로 갖고 있는 행렬이라 합시다.  
그러면 $EE^T = E^TE = I$ 가 됩니다. (대칭행령의 고유벡터들은 othonormal 이므로)  
그래서 orthrogonal 유닛벡터가 있습니다.

#### example 11.5 

행렬 E 가 

<img src="11.Dimensionality Reduction.assets/image-20200724160605220.png" alt="image-20200724160605220" style="zoom:50%;" />

이면, EE^T 는

<img src="11.Dimensionality Reduction.assets/image-20200724160626258.png" alt="image-20200724160626258" style="zoom:50%;" />

로 단위행렬이 나옵니다.

## 11.2 Pricipal-Component Analysis

PCA는 고차원 공간의 점을 나타내는 튜플 세트로 구성된 데이터 세트를 취하고 튜플이 가장 잘 정렬되는 방향을 찾는 기술입니다.   
이 아이디어는 튜플들의 집합을 행렬로 다루고, MM^T 나 M^TM 의  고유 벡터들을 찾습니다.  
이러한 고유벡터들의 행렬은 고차원 공간에서의 rigid rotation 으로 볼 수 있습니다.  
우리가 기존 데이터에서 이러한 변환을 적용하면, 주 고유벡터를 따른 축은 점이 가장 많이 퍼진 축입니다.  
더 정확하게 말하자면, 그 축은 데이터의 분산이 최대가 되는 점 입니다.  
다른 방식으로 보자면, 축에서 작은 편차를 가진 점들이 축에 따라 가장 잘 놓여 있다고 볼 수 있습니다.  
이와 같이 두번째 고유벡터를 따르는 축은 첫번째 축으로 부터의 분산이 가장 큽니다.

우리는 PCA를 데이터마이닝 기술로 볼 수 있습니다.  
고차원의 데이터는 그것의 가장 중요한 축의 projection으로 대체될 수 있습니다.  
이러한 축들은 가장 큰 고유값을 따릅니다.  
그래서, 처음 데이터는 더 낮은 차원인 데이터로 근사됩니다.

### 11.2.1 An Illustrative Example

우리는 단순한 예시로 시작합니다.   
이 예시는 2차원의 데이터입니다.  이 차원의 수는 PCA 를 유용하게 만들기에는 너무 작은 차원이긴 합니다.  
더욱이 그림 11의 데이터는 4개의 점만 갖고 있고, 단순하게 45도 의 각도의 패턴으로 놓여 있어서 계산하기 쉽게 만들어 졌습니다.  
기대되는 결과는 점들이 수직방향에서 작은 편차를 가지고 45도 각도의 축으로 놓여 있게 되는 것입니다.

<img src="11.Dimensionality Reduction.assets/image-20200724163838291.png" alt="image-20200724163838291" style="zoom:50%;" />

4개의 행을 가진 M을 다음과 같이 구성해 봅니다.

<img src="11.Dimensionality Reduction.assets/image-20200724163921581.png" alt="image-20200724163921581" style="zoom:50%;" />

M^TM 을 계산하면

<img src="11.Dimensionality Reduction.assets/image-20200724164040056.png" alt="image-20200724164040056" style="zoom:50%;" />

다음 식을 계산해서 고유값을 찾을 수 있습니다.

<img src="11.Dimensionality Reduction.assets/image-20200724164102057.png" alt="image-20200724164102057" style="zoom:50%;" />

여기서 고유값은 58, 2 입니다.  
고유값 중 하나인 58을 선택해 

<img src="11.Dimensionality Reduction.assets/image-20200724164148332.png" alt="image-20200724164148332" style="zoom:50%;" />

를 계산, 고유벡터를 얻어냅니다.

<img src="11.Dimensionality Reduction.assets/image-20200724164234389.png" alt="image-20200724164234389" style="zoom:50%;" />

2를 선택해

<img src="11.Dimensionality Reduction.assets/image-20200724164249975.png" alt="image-20200724164249975" style="zoom:50%;" />

<img src="11.Dimensionality Reduction.assets/image-20200724164314392.png" alt="image-20200724164314392" style="zoom:50%;" />

고유벡터를 계산합니다.

우리는 고유벡터의 첫번째 성분이 양수이기를 희망했지만, 여기서는 반대로 선택했습니다.  
이 상황에서는 좌표 변환이 더 쉽게 만들어주기 때문입니다.

이제 E를 구성해봅시다.  
첫번재 고유벡터를 앞에 두고 행렬을 만듭니다.

<img src="11.Dimensionality Reduction.assets/image-20200724165346418.png" alt="image-20200724165346418" style="zoom:50%;" />

othonormal 벡터들의 어떠한 벡터는 로테이션을 대표하고/하거나 유클리드 공간의 축들의 reflection 입니다.  
위의 행렬은 반시계의 45도 rotation 볼 수 있습니다.  

<img src="11.Dimensionality Reduction.assets/image-20200724165746742.png" alt="image-20200724165746742" style="zoom:50%;" />

예를들어, 우리가 행렬 M 을 곱한다면

<img src="11.Dimensionality Reduction.assets/image-20200724165649886.png" alt="image-20200724165649886" style="zoom:50%;" />

첫번째 점 [1.2] 은 $[3/\sqrt{2}, 1/\sqrt{2}]$  로 변환됩니다.

새로운 x 축을 나타내는 점선으로 그림 11.2를 살펴보면 첫 번째 점이 해당 축에 투영되어 원점에서 3 / √2 거리에있는 것을 볼 수 있습니다.   
[1,2] 가 투영된 [1.5,1.5] 이 원점에서 3 / √2  거리입니다.

또한 새로운 y 축은, 점선에 수직입니다.    
첫번째 점은 y축 방향에서 1/ √2   거리입니다. 첫번째 점에서 점선의 projection 을 빼면 y의 projcetion 이 나옵니다.

<img src="11.Dimensionality Reduction.assets/image-20200724171415390.png" alt="image-20200724171415390" style="zoom:50%;" />

두번째 점 [2,1] 에서도 동일하게 됩니다.

새로운 x,y 축으로 확인해보면 다음과 같이 보입니다.

<img src="11.Dimensionality Reduction.assets/image-20200724171906705.png" alt="image-20200724171906705" style="zoom:50%;" /> 



### 11.2.2 Using Eigenvectors for Dimensionality Reduction

예시로부터 수행해보았고, 일반적인 원리를 보았습니다.  
만약 M이 각 행이 유클리드 공간에서 어떤 숫자의 점으로 표현된다면, 우리는 $M^TM$ 와 그것의 고유쌍을 계산할 수 있습니다.  
E 를 고유값이 큰 순서대로 고유벡터를 나열한 행렬이라고 합시다.  
행렬 L 을 $M^TM$ 의 고유값을 갖고 대각에 따라 가장 큰 첫번째와 나머지 모든 0 을 갖는 행렬로 정의합니다.  
그러면   $M^TMe = \lambda e = e \lambda$ 이기 때문에 (각 고유값 \lambda 에 따른 고유벡터 e), 이것은 $M^TME = EL$ 이 됩니다.

우리는 ME 가 새로운 좌표 공간으로 변환된 M의 점이라는 것을 보았습니다.  
이 공간에서 첫번째 축은(최대 고유값을 따른) 가장 중요합니다. formally, 점들의 분산이 그 축에서 가장 큽니다.  
두번째 축은(두번재 고유쌍을 따른) 같은 맥락으로 그 다음으로 중요합니다. 이런 패턴이 고유쌍에 대해 이어집니다.  
만약 우리가 M 을 더 작은 차원으로 변환하기 원한다면, 가장 큰 고유값과 관련된 벡터를 보존하고 나머지는 무시하는 선택을 합니다. 

$E_k$ 를 E 의 처음 k 까지의 열이라고 한다면, $ME_k$ 는 M의 k 차원 representation 입니다.  

  

### 11.2.3 The Matrix of Distances

11.2.1. 에서의 예시를 봅시다, $M^TM$ 대신 $MM^T$ 를 검사해봅시다.   
예시 M이 칼럼보다 더 많은 행을 갖었기 때문에, 이는 더 큰 행렬이 만들어 집니다.   
그러나 만약 M 이 행보다 열이 더 많다면, 이는 더 작은 행렬이 될 것입니다.   
MM^T 를 계산해보면

<img src="11.Dimensionality Reduction.assets/image-20200724174847214.png" alt="image-20200724174847214" style="zoom:50%;" />

이것도 동일하게 대칭행렬입니다.  
여기 i 행 j 열에 대한 것은 쉽게 해석할 수 있습니다.  
i와 j 번째 점들로 표현된 벡터들의 dot product 입니다.

$M^TM,MM^T$ 의 고유값 사이는 밀접한 관계가 있습니다.   
e 가 M^TM 의 고유벡터라고 가정하면,  
$M^TMe = \lambda e$ 입니다.  
왼쪽에 M 을 곱하면 다음과 같습니다.

<img src="11.Dimensionality Reduction.assets/image-20200724175258827.png" alt="image-20200724175258827" style="zoom:50%;" />

그래서, Me 가 0 벡터가 아니므로, $MM^T$ 의 고유벡터와 $\lambda$ 는 $M^TM$ 의 고유값이 될 것입니다.

반대도 동일합니다.  
만약 e 가 $MM^T$ 의 고유벡터라면, $MM^T$ e = $\lambda e$ 로 시작해서, 왼쪽에 $M^T$ 를 곱합니다.  
그 결과는 $M^TM(M^Te) = \lambda (M^Te)$ 이고, $M^Te$ 가 0이 아니라면, $\lambda$ 는 $M^TM$ 의 고유값입니다. 

만약 $M^Te=0$ 인 경우를 보자면, $MM^Te$ 도 영벡터 입니다.  
그러나 영벡터는 고유벡터가 될 수 없기 때문에 e 는 영벡터가 아닙니다.  
그래서 $0 = \lambda e$ 라면 $\lambda= 0$ 입니다.

우리는 $MM^T$ 의 고유값들은 $M^TM$ 의 고유값들에 0을 더한 것이라고 결론지을 수 있습니다.   
만약 $MM^T$ 의 차원이 더 작으면, 그 반대도 참입니다. 

위의 예시에서 $MM^T$ 의 고유값은 58,2,0,0 이 됩니다.



## 11.3 Singular-Value Decomposition

SVD 는 어떤 행렬이든 정확한 representation 을 할 수 있게하고, 표상의 덜 중요한 부분을 쉽게 제거해서 원하는 만큼의 차원을 가진 approximate representation 을 만듭니다.  
더 작은 차원을 선택하면 근사치가 떨어지기는 합니다.

정의부터 시작하고 SVD가 행과 열을 잇는 개념들에 대해 정의하는 아이디어에 대해 알아봅니다.  
우리는 어떻게 가장 덜 중요한 개념을 제거해, 기존 행열에 근사하면서도 작게 만드는것을 하는 지 볼 것입니다.  
그리고 이 개념이 쿼리에 어떻게 효율적으로 사용될 수 있는지 볼 것입니다.



### 11.3.1. Definition of SVD 

M(mxn) 행렬이 있고, M의 랭크를 r로 둡시다.   
행렬의 랭크는 행의 nonzero 선형 조합이 아닌 것은 영벡터인 것을 선택했을 때, 행들에서(이나 열) 가장 큰 수 입니다.  
그래서 우리는 U, V, $\Sigma$ 를 다음과 같이 찾을 수 있습니다.  

<img src="11.Dimensionality Reduction.assets/image-20200725094316816.png" alt="image-20200725094316816" style="zoom:50%;" />

1. U 는 mxr 의 column-orthonomal matrix 입니다.   
   이것의 칼럼들은 각각 유닛벡터입니다. (각 칼럼끼리의 dot 이 0)
2. V 는 nxr 의 column-orthonormal matrix 입니다.  
   여기서 V 는 transpose 형태로 사용합니다.   
   그래서 $V^T$ 의 행은 orthonormal 입니다.
3. $\Sigma$ 는 diagonal matrix 입니다.  
   대각 성분 외의 모든 요소는 0 입니다.   
   이 행렬의 성분들은 M의 singular value 라고 합니다.

#### example 11.8

그림 11.6은 movie-user 레이팅에 관한 2랭크 행렬을 보여줍니다. 

<img src="11.Dimensionality Reduction.assets/image-20200725094546432.png" alt="image-20200725094546432" style="zoom:33%;" />

여기서 영화에 숨어있는 2개의 conncept 이 있습니다. : SF, 로맨스  
모든 남자들은 SF 에 평가를 했고, 여자들은 로맨스에만 평가를 했습니다.  
2의 행렬 랭크를 주는 컨셉을 고수하는 엄격함이 있습니다.  
우리는 아마 첫번째 4개의 중 한 행을 선택하고 뒤의 3개 중 한 행을 선택해서 이 행들의 nonzero 선형 합이 아닌 것이 0 인지 볼 것입니다.   
그러나 우리는 3개의 독립 행렬을 선택할 수 없습니다.  
예를들어 1,2,7 행을 선택하면 1에 3을 곱하고 2를 뺀 다음 7에 0을 곱한 것을 더하면 0 이 됩니다.

우리는 칼럼에 대해 유사한 observation 을 할 수 있습니다.   
우리는 아마 3개의 칼점 중 하나를 선택하고, 두 2개의 칼럼 중 하나를 선택할 것입니다. 그러면 2개의 독립을 찾을 수 있습니다.  

M의 decomposition 은 그림 11.7 과 같습니다. 계산은 11.3.6 에서 다룹니다.

<img src="11.Dimensionality Reduction.assets/image-20200725100043966.png" alt="image-20200725100043966" style="zoom:50%;" />



### 11.3.2 Interpretation of SVD

SVD 게 제공하는 것을 이해하는 것의 키는 USV의 r 행들을 표상된 comcept들로 보는 것입니다. (M 행렬에 숨겨진)   
11.8 의 예시에서, 이 컨셉들은 명확했습니다. SF와 로맨스 가 있었습니다.  
M의 행들을 사람으로 보고 열들을 영화로 봅시다.  
그러면 행렬 U 는 사람과 컨셉을 연결합니다.  
예를들어 M의 1 행은 Joe 이고 SF 컨셉만 좋아합니다.  
U 의 1행 1열의 값 0.14 는 칼럼의 다른 엔트리보다 작습니다.  
이는 joe 가 그 영화들을 다른 사람에 비해 점수를 낮게 주었기 때문입니다.    
두번째 칼럼의 경우 0 인데 이는 joe 가 로맨스 영화에 대한 평가를 하나도 안했기 때문입니다.

행렬은 V 영화-개념과 관련있습니다.   
$V^T$ 의 첫번째 부터 세번째 칼럼의 첫 행의 값 0.58 은 3개의 영화 매트릭스, 에일리언, 스타워즈를 가리킵니다.  
이것들은 SF 장르이고, 나머지 두 칼럼에서 0 은 이 영화들이 로맨스 개념에 대한 성질이 없다는 것을 말합니다.   

마지막으로, S 행렬은 각 개념들에 대한 강도를 나타냅니다.  
예시에서, SF 컨셉의 강도는 12.4 이고 로맨스의 강도는 9.5 입니다.  
직관적으로, SF 컨셉이 더 강합니다.  
데이터는 SF의 명화와 그 것을 좋아하는 사람들에 대한 정보를 더 많이 제공하고 있기 때문입니다.

일반적으로, 컨셉들은 명확하게 묘사되진 않습니다.  
S 가 항상 대각행렬이고 대각 외의 것들이 항상 0이 더라도, U와 V에 더 적은 0 이 있을 것입니다.  
M의 행과 열로 표현된 엔티티들은 더 다양한 컨셉들의 성질이 있을 것 입니다.  
사실, 11.8  의 decomposition 은 특히 단순합니다. 실제에서는 이렇게 단순하지 않습ㄴ디ㅏ.  
M의 랭크가 USV 행렬에 대해 원하는 칼럼의 수보다 더 크면, 그 분해는 정확해지지 않습니다.  
최적의 근사치를 얻기 위해, 우리는 정확한 분해로 부터 가장 작은 singular value 를 따르는 칼럼을 제거하는 것이 필요합니다.   
다음 예시는 11.8을 약간 수정한 것입니다.

<img src="11.Dimensionality Reduction.assets/image-20200725104219769.png" alt="image-20200725104219769" style="zoom:50%;" />

#### example 11.9

<img src="11.Dimensionality Reduction.assets/image-20200725105409551.png" alt="image-20200725105409551" style="zoom:50%;" />

Jill 과 Jane 이 에일리언에 점수를 매겼습니다.  
이전에는 rank 2 였지만 이제 3이되었습니다.  
S 에서 첫번째는 SF, 두번째는 로맨스이고  
3번째의 컨셉은 설명하긴 어렵습니다. 그러나 이것은 그렇게 중요하진 않습니다. weight 가 작습니다.

다음 세션에선 덜 중요한 개념을 제거하는 법에 대해 다룹니다.

### 11.3.3 Dimensionality Reduction Using SVD 

엄청 큰 행렬 M 을 USV 로 표현하는 작업을 가정해 봅시다.  
그러나 행렬이 너무커서 쉽게 저장할 수 없습니다.  
이 세개의 행렬을 차원적으로 줄이는 최고 의 방법은 가장 작은 특이값을 0으로 바꾸는 것입니다.  
s 의 작은 특이값을 0으로 바꾸면 이에 따르는 U, V의 s 칼럼들을 제거할 수 있습니다.

#### example 11.10 

11.9 예시는 3개의 특이값을 갖었습니다. 여기서 2개의 차원으로 줄이고 싶습니다.   
그러면 우리는 가장작은 특이값 1.3 을 0 으로 바꿉니다.   
U의 3번째 칼럼과 $V^T$ 의 3번째 행은 연산시 0만 곱해지므로, 이 행과 칼럼은 없어도 됩니다.  
M' 에 근사치는 두개의 특이값만 사용해서 얻을 수 있습니다. 

<img src="11.Dimensionality Reduction.assets/image-20200725111146983.png" alt="image-20200725111146983" style="zoom:50%;" />

이상적으로 전체 차이는 마지막 특이값을 0으로 만든 결과입니다.   
그러나 이는 단순한 예시이고, 더 차이가 있습니다. 그 차이의 대부분은 M '의 분해가 두 개의 유효 자릿수로만 정확했기 때문에 발생하는 반올림 오차로 인한 것입니다.



### 11.3.4 Why Zeroing Low Singular Values Works

작은 특이값을 버리는 선택은 M과의 rmse 를 최소화하는 것을 보여줄 수 있습니다.  
엔티티의 수는 고정되어 있고 스퀘어루트는 단조로운 연산(monotone)이기 때문에, 우리는 단순화 할 수 있고 행렬과 관련된 Frobenius norm 을 비교할 수 있습니다.  
M 이 그것의 근사치와 차이가 있다면 ||M|| 은 그 행렬들 사이의 rmse 에 비례합니다.

왜 가장 작은 특이값을 0 으로 만드는 것으로 rmse 를 최소화 하는지 설명하기 위해서, 작은 행렬 대수부터 시작합시다.  
M 이 3개의 행렬의 곱(product)이라고 가정해 봅시다. M = PQR, $m_{ij},p_{ij},q_{ij},r_{ij}$ 는 각 행렬의 성분입니다.  
그러면 행렬 곱의 정의에 따라 다음이 나옵니다.  

<img src="11.Dimensionality Reduction.assets/image-20200725112854558.png" alt="image-20200725112854558" style="zoom:50%;" />

그러면

<img src="11.Dimensionality Reduction.assets/image-20200725112920090.png" alt="image-20200725112920090" style="zoom:50%;" />

우리가 텀의 합의 제곱을 할 때, 식 11.1 의 오른쪽 처럼,  우리는 효율적으로 합의 두개의 카피들을 만들 수 있고 첫번째 합의 각 텀을 두번째 합의 각 텀에 곱할 수 있습니다.

<img src="11.Dimensionality Reduction.assets/image-20200725113231418.png" alt="image-20200725113231418" style="zoom:50%;" />

<img src="11.Dimensionality Reduction.assets/image-20200725113556306.png" alt="image-20200725113556306" style="zoom:50%;" />

이제 PQR을 $U\Sigma V^T$ 라고 합시다.   
R은 row-orthonomal 이고, 그것의 행들은 유닛벡터입니다, 그리고 행의 dot product 는 0이 됩니다.  
Q가 대각행렬이기 때문에 k=l, n=m 일때 빼고는 $q_{kl} ,q_{nm}$  은 0이 됩니다.  그래서 l 과 m 에대한 서메이션은 빼도 됩니다.  그래서 다음과 같이 됩니다.

<img src="11.Dimensionality Reduction.assets/image-20200725114525029.png" alt="image-20200725114525029" style="zoom:50%;" />

다음 서메이션을 재순서합니다. 그래서 i 가 내부에 있도록 합니다.  
11.3 은 $p_{ik} p_{in}$ 만 i 와 관련이 있습니다.  모든 다른 항들은 상수로 보고 i 에 관한 서메이션만 신경습니다.  
P 가 column-orthonormal 이기 때문에, k 가 n 일때  $\Sigma_ip_{ik}p_{in} = 1$ 이고 그외에는 0 입니다.  
그러므로 k=n 으로 설정하고  $p_{ik} p_{in}$ 항을 버릴 수 있습니다. 다음과 같이 줄일 수 있습니다.

<img src="11.Dimensionality Reduction.assets/image-20200725115122720.png" alt="image-20200725115122720" style="zoom:50%;" />

R은 row-orthonormal 이므로 $\Sigma_jr_{kj}r_{kj} = 1$ 입니다. 그래서 $r_{kj}$ 항과 j 에 대한 합을 제거할 수 있습니다.  
그래서 다음과 같이 단순한 식을 만들 수 있습니다.  

<img src="11.Dimensionality Reduction.assets/image-20200725115436133.png" alt="image-20200725115436133" style="zoom:50%;" />

M - M' = $U(S - S')V^T$ 이므로   
$||M - M'||^2$ 를 S-S' 의 대각 요소의 제곱합과 동일하게 볼 수 있습니다.  
이를 최소화 하기 위해서는 특이값에서 가장 작은 요소를 선택해야합니다.

#### How Many Singular Values Should We Retain?

유용한 rule of thumb(경험법칙) 은 S의 energy 의 90%으로 특이값을 유지하는 것입니다.  
이것은 유지한 특이값의 제곱합이 적어도 전체 특이값의 제곱합의 90%는 되어야 한다는 의미입니다.  
11.10 예시에서는 $12.4^2 + 9.5^2 + 1.3^2 = 245.70$ 이 전체 energy이고, 유지한 energy 는 $12.4^2  + 9.5^2 = 244.01$ 으로 99% 입니다.

### 11.3.5 Querying Using Concepts

이 세션에서 우리는 어떻게 SVD 가 특정 쿼리에 대해 효율적이고 좋은 정확도로 답을 하는지에 대해 살핍니다.   
우리는 그림11.7 의 SVD 형태로 구성된 기존 영화 평가 데이터를 분해 한 것을 갖는다고 가정해봅시다.

A라는 사람이 매트릭스를 보고 4점을 메겼다고 합시다. 그러면 이 사람의 벡터는 q=[4,0,0,0,0] 입니다.  

만약 CF 방식을 사용한다면, 우리는 A 를 기존 행렬에 있는 다른 사람들과 비교를 합니다.   
대신, 우리는 A 를 행렬 V 에 곱하여 concept space 에 매핑할 수 있습니다.  
우리는 qV = [2.32, 0] 이라는 것을 찾을 수 있습니다.  
이것은 A 가 SF 에 관심이 있다는 것을 의미합니다.  

우리는 이제 컨셉 공간에서 A를 표현했지만 원래의 movie space에서 파생 된 표현과는 다릅니다.  
우리가 그의 표상을 영화 공간에 매핑하는 유용한 방법은 [2.32,0] 을 $V^T$ 와 곱하는 것입니다.   
이는 [1.35,1.35,1.35,0,0] 입니다. 이는 A 에게 에일리언, 스타워즈를 제시합니다.

우리가 개념 공간에서 수행할 수 잇는 다른 쿼리의 종류는, A 와 유사한 사용자를 찾아주는 것입니다.  
우리는 V 를 컨셉공간의 모든 사용자에게 매핑하는데 사용할 수 있습니다.  
예를들어, Joe 는 [1.74,0] 으로 Jill 은 [0, 5.68] 으로 맵이 됩니다.  
이렇게 컨셉 공간의 사용자 정보는 코사인 거리를 사용하여 사용자 간 유사도를 측정할 수 있습니다.  
예시에서는 A 가 Joe 와 유사합니다.(0) Jill 과는 코사인 거리가 1 이됩니다.   

### 11.3.6 Computing the SVD of a Matric

생략!





## 11.4 CUR Decomposition

SVD 가 가진 문제는 11.3 에서 보이지 않습니다. 대규모 데이터 어플리케이션에서, 행렬 M 은 매우 sparse 하게 구성되는것 이 일반적입니다. (대부분이 0 )   
예를 들어 문서들과 그 안에 있는 단어를 표현한 행렬는 sparse 합니다.   
대부분의 단어들은 문서들에서 나타나는 것은 아니기 때문입니다.   
유사하게, 고객과 제품의 행렬도 sparse 합니다.   
많은 사람들이 대부분의 제품을 사는게 아니기 때문입니다.

우리는 dense 행렬들을 다루지 못합니다.(백만 이상의 행, 칼럼을 갖는)  
그러나 SVD 에서 M 이 sparse 하더라도 U와 V 가 dense 합니다.  
S가 대각이기 때문에, 그것이 희소하게 될 것이지만 그것은 U와 V 보다 일반적으로 더 적습니다.   
그래서 그서들의 sparseness 에 도움이 되지 않습니다. 

여기서는, CUR-decomposition 이라는 다른 분해 방식을 다룹니다.  
이 방식은 M 이 희소하고, SVD 에서 U,V 와 유사한 두개의 큰 행렬 또한 희소할 때, 메리트가 있습니다.  
중간에 있는 행렬만 dense 하지만, 이 행렬은 작아서 처리에 큰 어려움이 없습니다.

파라미터 r 을 취해서 정확한 분해를 하는 SVD 와 달리, CUR 분해는 만드는 r이 얼마나 큰지는 중요하지 않은 근사입니다(approximation).  
r이 커짐에 따라 M에 대한 수렴을 보장하는 이론이 있지만 일반적으로 r 을 너무 커야 1 % 이내로 되고, 이 방법은 실용적이지 않습니다.  
그럼에도 불구하고, 상대적으로 작은 r 로 분해하는 것은 유용하고 정확한 분해가 될 가능성이 높습니다.



### 11.4.1 Definition of CUR

M을 m행과 n 칼럼의 행렬이라고 둡시다.  
컨셉의 목표 수 r 을 정합니다.(분해에 사용될)  
M 의 CUR 분해는 M의 r개의 칼럼의 무작위적으로 선택된 집합, C 행렬(mxr) 과   
M의 r개의 행이 무작위로 선택된 집합, R(rxn) 행렬 입니다.  
rxr 의 행렬 U는 C와 R으로 부터 구성됩니다.

1. W 를 rxr 행렬, C행렬과 R행렬에서 교차되는 집합이라고 봅시다.  
   W의 i 행과 j 칼럼은 C의 j 칼럼과 R 의 i 행입니다.
2. W 에 대한 SVD 를 계산합니다.  $W = X\Sigma Y^T$
3. $\Sigma^+$ 를 계산합니다. 대각 행렬의 Moore-Penrose pseudoinverse  입니다.  
   만약 i 번째 요소 $\sigma$ 가 0이 아니라면, 그것을 $1/\sigma$ 로 바꾸고,   
   0 이라면 0으로 남겨둡니다.
4. U = $Y(\Sigma^+)^2 X^T$ 라 합니다.

우리는 11.4.3. 예시에서  CUR 의 전체 과정을 다룹니다.  
여기서 어떻게 C와 R 을 M 과 가까워지도록 선택하는지도 다룹니다.

####  Why the Psedoinverse Works

행렬 M 은 XZY 와 동일하다고 봅시다.  
만약 모두 역합수가 존재한다면, $M^{-1} = Y^{-1}Z^{-1}X^{-1}$ 입니다.    
XZY 가 SVD 인 경우에 관심이 있기 때문에, X 가 column-orthonormal 이고 Y 가 row-orthonormal 이라는 것을 알고있습니다.   
이 경우 역함수와 전치가 같습니다.  
$XX^T, YY^T$  가 모두 단위행렬이 됩니다.  
그래서 $M^{-1} = Y^TZ^{-1}X^T$ 입니다.

우리는 이미 Z 가 대각행렬이라는 것을 알고 있습니다.  
만약 대각에 0 이 없다면, $Z^{-1}$ 는 Z 로부터 각 성분의 역수를 취해서 만들 수 있습니다.  
Z의 대각선을 따라 0이있을 때만 역의 동일한 위치에 대한 요소를 찾을 수 없으므로 Z에 역을 곱할 때 항등 행렬을 얻을 수 있습니다.  
그래서 pseudoinverse 를 사용합니다.  
이는 ZZ+가 항등 행렬이 아니라 Z의 i 번째 요소가 0이 아닌 경우 i 번째 대각선 항목이 1 이고,  
Z의 i번째가 0일 때 0 인 행렬입니다.



### 11.4.2 Choosing Rows and Columns Properly

우리는 행과 칼럼을 무작위로 선택했습니다.  
그러나 이 선택은 편향되어야 합니다. 더 중요한 행과 열은 선택되는 더 나은 기회를 갖어야 합니다.  
우리가 반드시 사용해야하는 중요도의 측도는 프로비어스 놈의 제곱입니다. (행, 열의 요소의 제곱 합)  
$f= \Sigma_{i,j}m_{ij}^2$ 라 합시다.  
그러면 한 행을 선택할 때 행 i 를 선택하는 확률 $p_i$ 는 $\Sigma_jm_{ij}^2 / f$ 이고  
한 열을 선택할 때 열 j 를 선택하는 확률 $q_j$ 는 $\Sigma_im_{ij}^2 / f$ 입니다.  

#### example 11.12

<img src="11.Dimensionality Reduction.assets/image-20200725205221887.png" alt="image-20200725205221887" style="zoom:50%;" />

위 행렬에서 제곱합은 243 입니다.   
첫번째부터 세개의 칼럼의 Forbenius norm 은 $1^2+3^2+4^2+5^2 =  51$ 로 동일하고   
그래서 각 확률은 51/243 = 0.210 입니다.  
나머지 두 칼럼의 경우 $4^2 + 5^2 + 2^2 = 45$,  
각 45/245 = 0.185 의 확률을 갖습니다.  

이제 C에 쓸 r 개의 칼럼을 선택해 봅시다.  
각 칼럼은, M의 칼럼으로 부터 무작위로 선택합니다.  
그러나 선택은 같은 확률을 같지 않습니다.  대신 j 칼럼이 $q_j$ 의 확률을 갖습니다.  
M의 칼럼으로부터 독립적으로 C 의 칼럼이 선택되므로, 칼럼은 한번 이상 선택될 수 있습니다.  
우리는 이러한 상황은 CUR 분해의 기초를 설명하고 난 후 다룹니다.

M의 선택된 칼럼을 갖고, 각 열의 요소를이 열을 선택하는 예상 횟수의 제곱근으로 나누어 각 열의 크기를 조정합니다.  
우리는 j 번째 칼럼의 요소들을 $\sqrt{rq_j}$ 로 나눕니다. M의 조정된 칼럼은 C 의 칼럼이 됩니다.

R도 동일한 방식으로 선택합니다.  
각 행은 i 행의 확률 $p_i$ 를 가지고 하고,  
pi는 i 번째 행의 요소의 제곱의 합을 M의 모든 요소의 제곱의 합으로 나눈 값입니다. 그런 다음 선택한 각 행을 나누어서 스케일합니다.
 선택한 M의 i 번째 행인 경우 $\sqrt{rp_i}$로.@

#### example 11.13 

r=2 로 두고, 11.12 그림의 행렬에서 에일리언(2번째), 카사블랑카(4번째) 를 무작위로 뽑았다고 가정해 봅시다.   
아일리언에 대한 칼럼은 [1,3,4,5,0,0,0]^T 이고 이 칼럼은 $\sqrt{rq_2}$ 로 나눠서 스케일해야합니다.  
이전 예시에서 $q_2 = 0.210$ 으로 계산하였으므로, $\sqrt{rq_2} = 0.648$ 이 됩니다.  
그래서 스케일된 칼럼은 [1.54,4.63,6.17,7.72,0,0,0]^T 가 됩니다. 이것이 C의 첫번째 칼럼입니다. 

C 의 두번째 칼럼은 $\sqrt{rp_4} = \sqrt{2\times 0.185} = 0.608$ 을 사용하여 스케일을 합니다.   
그래서 C의 두번째 칼럼 [0,0,0,0,6.58,8.22,3.29]^T  를 얻습니다.

행의 경우도 동일하게 스케일하여 다음과 같은 R 행렬을 얻을 수 있습니다.

<img src="11.Dimensionality Reduction.assets/image-20200725224547682.png" alt="image-20200725224547682" style="zoom:50%;" />



### 11.4.3  Constructing the Middle Matrix

마지막으로, 우리는 U 행렬을 구성해야합니다. 이는 rxr 행렬입니다.  
우선 같은 크기의 다른 행렬 W 를 생성하는 것부터 시작합니다.  
W의 행 i 와 열 j 는 M 에서 무작위로 선택한 행과 열이고 이는 C와 R 을 구성하였습니다.  

#### example11.14 

11.13 예시에서 행과 열을 다음과 같이 선택했다고 봅시다.

<img src="11.Dimensionality Reduction.assets/image-20200725205221887.png" alt="image-20200725205221887" style="zoom:50%;" />

<img src="11.Dimensionality Reduction.assets/image-20200725233822693.png" alt="image-20200725233822693" style="zoom:50%;" />

W의 첫번째 행은 R의 첫번째 행을 따르고, 이는 M의 Jenny 에 대한 행입니다.  
첫번째 행의 첫번째 칼럼이 0이고 두번째 칼럼이 5이므로 Jenny 입니다.  
두번째 행은 Jack 입니다. 앞에는 5이고 뒤에는 0 입니다.   
그리고 칼럼은 에일리언과 카사블랑카 입니다.

행렬 U 는 W 에서 수도인버스를 사용해 만들어진 행렬입니다.  
그것은 W 의 SVD 를 취하는 것으로 구성됩니다.  
$W = X \Sigma Y^T$ 일때, $\Sigma $ 의 수도 인버스를 $\Sigma^+$ 라 하고(nonzero 를 역수로 취하는 것)  
$U = Y(\Sigma^+)^2 X^T $ 를 수행하면 됩니다.

#### example 11.15

W 를 SVD 분해를 하면 다음과 같습니다.

<img src="11.Dimensionality Reduction.assets/image-20200725234626855.png" alt="image-20200725234626855" style="zoom:50%;" />

여기서 pseudoinverse $\Sigma^+$ 는 다음과 같습니다.

<img src="11.Dimensionality Reduction.assets/image-20200725234706788.png" alt="image-20200725234706788" style="zoom:50%;" />

그러므로 행렬 U 는 다음과 같습니다.

<img src="11.Dimensionality Reduction.assets/image-20200725234727488.png" alt="image-20200725234727488" style="zoom:50%;" />

### 11.4.4 The Complete CUR Decomposition

우리는 이제 무작위로 선택해서 3개의 성분 행렬 C, U R 을 갖었습니다.  
그것들의  product 는 기존 행렬 M 에 근사합니다.  
시작할 때 언급한 것 과 같이, 이 근사치는 일반적으로 행과 열이 매우 많이 선택되었을 때만 가까워지는 것을 보장합니다.  
그러나, 행과 열을 높은 중요도의 경향을 가지고 선택하는 직관은 작은 수의 행과 열을 가지고도 기존 행렬의 가장 중요한 부분을 추출하게 합니다.  
우리는 어떻게 잘 동작하는지 예를 통해 봅시다.

#### example 11.16 

<img src="11.Dimensionality Reduction.assets/image-20200725235413528.png" alt="image-20200725235413528" style="zoom:33%;" />

이전까지의 예시에서, decomposition 은 위 그림처럼 수행됩니다.  
실제 값과 이 결과의 차이는 크지만(특히 SF의 숫자에서), 그 값은 기존의 것과 비례합니다.  
이 예시는 너무 작지만 행과 열의 작은 수 선택은 랜덤보다는 인위적이여서  
우리에게 CUR 분해가 실제 값에 가까히 수렴하는 것을 예상할 수 있습니다.

### 11.4.5 Eliminating Duplicate Rows and Cloumns

단일 행과 열이 한번 이상 선택될 가능성이 있습니다.  
같은 행을 두번사용하는 것은 그리 위험하진 않습니다. 비록 decomposition 의 행렬들의 랭크가 선택된 행, 열의 수보다 작아질 것일지라도  
그러나 이 또한 가능합니다. R의 k 행들을 합치는 것이 가능합니다. M의 같은 행들을 R의 단일 행으로,  
그래서 R은 더 적은 행으로 남깁니다.  
이와 같이 C 도 합쳐서 C 에서 더 적은 칼럼으로 만들 수 있습니다.  
그러나 행들과 칼럼들에 대해 남아있는 벡터는 그것의 각 성분에 $\sqrt{k}$ 를 곱해야합니다.  

우리가 행이나칼럼을 합칠 때, R의 행이 C 의 칼럼보다 더 적을 수 있고, 그 반대도 될 수 있습니다.  
그 결과 W 는 정방행렬이 될 수 없게 됩니다.  
그러나 우리는 그것의 수도인버스를 취할 수 있습니다.   
$\Sigma$ 에 전치를 시키고 nonzero 의 역수를 취하면 수도인버스, $\Sigma^+$ 를 구할 수 있습니다.  

<img src="11.Dimensionality Reduction.assets/image-20200726000922349.png" alt="image-20200726000922349" style="zoom:50%;" />