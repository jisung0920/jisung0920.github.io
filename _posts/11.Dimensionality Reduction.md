## 11. Dimensionality Reduction

데이터를 큰 행렬로 볼 수 있는 많은 소스가 존재합니다.   
Chap 5 에 transition matrix, 10에서 소셜 네트워크 등..  
그 행렬들은 기존과 유사한 'narrower' 행렬을 찾아서 요약할 수 있습니다.   
이 'narrower' 행렬은  행이나 열이 더 작아서 더 효율적으로 사용할 수 있습니다.

이 단원에서는 차원 축소의 아이디어를 좀 더 자세하게 다룹니다.  
PCA 에서 고유값에 대한 사용에 대해 다루고,  
UVD의 더 강력한 버전인 SVD 에 대해 다룹니다.  
마지막에는 대규모 데이터에서 사용하는 CUR 이라는 다른 형태의 분해 방식을 다룹니다. 이는 SVD의 변종입니다. (sparse 를 유지하면서 분해하는 방법)



## 11.1 Eigenvalues and Eigenvectors of Symmetric Matrices

### 11.1.1 Definitions

L 을 정방 향렬, $\lambda$ 를 상수, e 를 행에서 동일한 숫자를 갇는 nonzero 열 벡터라 합시다.    
$Me = \lambda e$  라면, $\lambda$ 를 M의 고유값, e 를 고유벡터라고 합니다. 

만약 e 가 고유벡터이고 c 가 아무 상수이면, ce 도 동일한 고유값을 갖는 고유벡터입니다.  
상수를 곱하는 것은 벡터의 길이는 바꾸지만 방향을 바꾸지는 않습니다.  
그래서 길이에 따라 애매한 것을 피하기 위해, 모든 고유벡터를 유닛 벡터로 해서 다룹니다. (성분의 제곱합이 1인 것 )  
이것은 고유벡터를 유일하게 만들기엔 조금 부족합니다, -1 을 곱한다면, 제곱합이여도 반대 방향의 벡터가 나와서 유일하지 않게됩니다.  
그래서 일반적으로 고유벡터의 첫번째 nonzero 성분이 양수가 되도록 요구합니다.

### 11.1.2 Computing Eigenvalues and Eigenvectors

우리는 이미 고유쌍을 찾는 방식에 대해 보았습니다. $M^iv$ 를 수렴할 때 까지 반복하는 것으로  
M 이 확률벡터일 떄, limiting 벡터는 principal 고유벡터(제일 큰 고유값을 갖는 벡터)이고 이는 고유값 1을 갖었습니다.  
이런식으로 고유벡터를 찾는 것은 (power iteration) 