## 12.3 SVM

퍼셉트론의 향상버전으로 12.2.7에 언급한 문제들을 다룰 수 있습니다.  
SVM 은 하나의 특정 초평면을 선택해서 점들을 두개의 클래스로 나눌 뿐 아니라 마진 을 최대화 합니다.  
마진 - 초평면과 훈련셋의 가장 가까운 점들 사이의 거리 

이번 세션에서는 SVM 의 분리할 수 있는 훈련 포인트들에 대해서 다루고,  
어떻게 그 경우 마진을 최대화 하는지에 대해 볼 것입니다.  
우리는 그러고 선형적으로 분리할 수 없는, 복잡한 문제에 대해 다룹니다.  
이 경우 목표가 다릅니다. 우리는 두개의 클래스를 가장 잘 나누는 초평면을 찾아야 합니다.  
그러나 best 는 트릭키한 개념입니다.  
우리는 손실함수를 개발하여 잘못 분류된 점들에 대해 패널티를 주고,  
또한 분류는 잘 되었지만 초평면에 너무 가가운 점들은 어느 정도 패널티를 줍니다. 이 문제는 12.3.3 에서 다룹니다. 



### 12.3.1 The Machanics of an SVM

SVM 의 목표는 초평면과 훈련셋의 점 사이의 거리 $\gamma$ 를 최대화 하는 초평면 wx+b = 0 을 선택하는 것입니다.   
다음과 같이 우리는 두 클래스의 점들과 그서들을 나누는 초평면을 볼 수 있습니다. 

<img src="12.Large-Scale Machine Learning.assets/image-20200727205613348.png" alt="image-20200727205613348" style="zoom:50%;" />

직관적으로, 초평면으로 부터 더 먼 점이 초평면과 가까운 점들보다 분류가 더 확실합니다.  
그래서 초평면이 가능한 멀리 있도록 포인트들을 두는 것이 바람직합니다.  
마진이 가장 크게만드는 초평면을 만드는 것의 이점은 훈련세트에는 없지만 전체 데이터 셋안에 초평면에 좀 더 가까이 있는 점이 있다는 것입니다.  
그렇다면 훈련 지점을 분리했지만 일부 지점이 초평면 자체에 매우 가까운 초평면을 선택한 경우보다 이러한 지점이 올바르게 분류 될 가능성이 더 큽니다.  
이 경우, 초평면에 가까운 훈련 지점에 가까운 새로운 지점이 잘못 분류 될 가능성이 있습니다.

우리는 또한 위 그림에서 초평면에서 감마거리의 초평면들을 볼 수 있습닏.ㅏ 그리고 이것들은 하나 이상의 support vector 와 닿습니다.  
후자는 실제로 초평면으로부터 γ 거리에 있다는 점에서 분할 초평면을 제한하는 점입니다.   
대부분의 경우 점들의 d 차원의 집합은 d+1 개의 support vector 들을 갖습니다. (위 그림과 같이)  
그러나 평행 초평면들에 점들이 너무 많이 있으면 더많은 서포트 벡터들이 있을 수 있습니다. 

현재 목표에 대해 다음과 같이 정리할 수 있습니다.  

- 훈련 셋 $(x_1,y_1),(x_2,y_2), \dots, (x_n,y_m)$ 이 주어졌을 때 다음 제약에 따라 $\gamma$ 를 최대화한다.  
  $y_i(w x_i +b) \geq \gamma$

y는 +1 이나 -1 이 되어야 하고, 이는 x가 초평면에 어느 쪽에 있어야하는지 결정합니다.

불행이도, 이 공식은 적절히 동작하지 않습니다.  
이 문제는 w 와 b 가 커짐에 따라 우리는 감마의 값을 크게 허용할 수 있습니다.  
예를들어 w,b 가 위의 조건을 만족할 때 $y_i(2wx_i+2b)\geq 2\gamma $ 도 성립하게되고, 이렇게 하면 최대 감마값을 선택할 수 없게 됩니다.

### 12.3.2 Normalizing the Hyperplane

앞서 언급한 문제의 답은 가중치 벡터 w 를 정규화(normalize) 하는 것입니다.  
그것은, 분리 초평면에 수직인 측정의 유닛은 유닛 벡터 w/||w|| 입니다. (Frobenius norm)  
우리는 요청한다 w 과 평행 초평면처럼 되기를 (서포트 벡터들의 건들기만 하는) $wx+b =1, -1$ ,  
다음 그림과 같이

<img src="12.Large-Scale Machine Learning.assets/image-20200727213056470.png" alt="image-20200727213056470" style="zoom:50%;" />

우리는 위와 같이 1,-1로 만든 식을 upper, lower 초평면이라고 부릅니다. 

이것은 감마가 1 인 것 처럼 보입니다. 그러나 우리는 w 를 유닛 벡터로 사용하기 때문에 감마는 'units'의 수 입니다. 즉, 분리 초평면과 평행 초평면 사이를 이동하는 데 필요한 방향 w의 스텝 입니다.  
우리의 목표는 유닛 벡터 $w/||w||$ 의 곱인 $\gamma$ 를 최대화 하는 것입니다.

우선 감마를 최대화 하는 것이 ||w|| 를 최소화 하는 것과 같다는 것을 증명합니다.  
위 그림의 서포트 벡터 x2 를 봅시다. x1은 upper 초평면에 x2 의 projection 이라고 봅시다.  
x1은 서포트 벡터나 트레이닝 셋의 점이 될 필요가 없습니다.  
w/||w|| 단위로 x2에서 x1 까지의 거리는(x1-x2) 2r 입니다.   
그러므로  
$$ x_1 = x_2  + 2\gamma \frac{w}{||w||}$$ 입니다. 

x1 은 초평면 wx+b = 1 상에 있으므로 wx1 + b = 1 이 성립합니다.  
위 식을 대입하면  

$$w(x_2 + 2\gamma \frac{w}{||w||}) + b = 1 $$ 을 얻고, 다시 정리하면 

$$wx_2 + b + 2\gamma \frac{ww}{||w||}=1$$, 여기서  $wx_2+b$ 는 -1 이므로  

$$\gamma \frac{ww}{||w||} = 1$$ 입니다.

ww 은 w 의 제곱합 이므로 $ww =||w||^2$ 입니다.  
이를 가지고 다음과 같은 결론을 낼 수 있습니다. 

$$\gamma = \frac{1}{||w||}$$

그러므로 감마를 최대화 하는 대신,$||w||$ 을 최소화 하는 최적화 문제로 바꿀 수 있습니다. 

- 훈련 셋 $(x_1,y_1),(x_2,y_2), \dots, (x_n,y_m)$ 이 주어졌을 때 다음 제약에 따라 $||w||$를 최소화한다.  
  $y_i(w x_i +b) \geq 1$

#### example 12.8 

다음과 같은 훈련데이터가 주어졌을 때 를 고려해봅시다.

<img src="12.Large-Scale Machine Learning.assets/image-20200727220653014.png" alt="image-20200727220653014" style="zoom:50%;" />

w= [u,v] 로 두고 목표는 위의 제약안에서 $\sqrt{u^2+v^2}$ 를 최소화 하는 것입니다.   
첫번째 경우, $(1)(u+2v+b) = u+2v+b \geq 1$ 
    
두번째, $(-1)(2u+v+b)\geq 1 $    
세번째,  $(1)(3u+4v+b)\geq 1 $   
네번째,  $(-1)(4u+3v+b)\geq 1 $   
의 식을 구성할 수 있습니다.  

위의 제약하에서 어떻게 최적화 하는지에 대해 구체적으로 다룰 것입니다. 이 주제는 광범위하고 사용가능 한 많은 패키지들이 있습니다. 12.3.4에서 분리 초평면이 없는 보다 일반적인 SVM 적용과 관련하여 그레디언드 디센트로 다룹니다. 

위의 예시 상황에서는 4개의 조건을 정확히 만족할 수 있지만 이 경우는 일반적이지 않습니다.  
2차원 데이터가 있을 때 우리는 3개의 서포트 벡터만 기대할 수 있기 때문입니다.  
그러나 + 인 예와 - 인 예가 평행선에 있다는 사실은 네 가지 제약 조건을 모두 정확하게 충족시킬 수 있습니다.

### 12.3.3 Finding Optimal Approximate Separators

우리는 일반적인 경우에 최적의 초평면을 찾는 것을 고려해봅니다.  
우리가 어떤 초평면을 선택하는 것과 상관없이 몇개의 점들은 잘못된 위치에 있을 것이고,  
아마 일부 점은 위치는 맞지만 분리 초평면에 너무 가까이 있을 수 있어서 margin 조건을 만족하지 않을 수 있습니다.  
일반적인 상황은 그림 12.16 과 같습니다. 

<img src="12.Large-Scale Machine Learning.assets/image-20200727223346746.png" alt="image-20200727223346746" style="zoom:50%;" />

우리 2개의 오분류를 볼 수 있습니다. 또 분리 초평면에 너무 가까운것을 볼 수 있습니다.  
이러한 점들을 bad 포인트라고 부릅니다.

각 bad 포인트들은 패널티가 발생합니다. 우리가 가능한 초평면을 평가 할 때.  
최적화의 부분으로서 결정되는 단위들에서 패널티의 양은  
그림과 같이, 초평면에서 bad 포인트들이 있는 곳까지의 화살표로 보여집니다.   
이 화살표들은 초평면 wx+b=1 이나 wx+b=-1 로부터의 거리를 측정합니다.  
전자는 분리 초평면 위 (예 : 레이블 y가 1이므로) 위에 있어야하는 훈련 예의 기준선이고, 후자는 아래 (예 : y = -1이므로)에 대한 점의 기준선입니다.

최소화 하는 공식에 관련한 많은 옵션들이 있습니다.  
직관적으로, 우리는 ||w|| 이 가능한 작기를 원합니다.  그러나 또한 우리는 bad 포인트들과 관련된 패널티들이 최대한 작기를 원합니다.  
tradeoff 의 일반적인 형태는 $||w||^2/2$ 텀과 패널티들의 상수배의 텀에 대한 수식으로 표현됩니다. 

$||w||^2/2$ 를 최소화 하는 것이 왜 적합한지 봅시다.  
$||w||$ 를 최소화 하는 것은 어떤 단조함수 $||w||$ 를 최소화 하는 것과 동일합니다.  
그래서 적어도 ∥w∥2 / 2를 최소화하려고하는 공식을 선택하는 옵션입니다.  
이 식은 바람직합니다. 왜냐면 w의 어떤 성분과 관련된 이것의 미분은 그 성분이기 때문입니다.  
만약 $w = [w_1,w_2,\dots,w_d] $ 라면 $||w||^2/2$ 는 $\frac{1}{2}\sum_{i=1}^{n}w_i^2$ 이고, 이것의 편미분은 $\partial / \partial w_i$ 은 $w_i$ 입니다.  
이 상황은 적합합니다. wi에 대한 페널티 항의 미분은 각 xi의 상수 시간이며, 훈련 예에서 페널티가 발생하는 각각의 feature 벡터의 대응 성분이다.  
이는 벡터 w 와 훈련 세트의 벡터가 그 구성 요소의 단위에 상응한다는 것을 의미한다.  
그래서 우리는 다음 함수를 어떻게 최소화할지 고려하면 됩니다.

<img src="12.Large-Scale Machine Learning.assets/image-20200727230804336.png" alt="image-20200727230804336" style="zoom:50%;" />

첫번째 텀은 ||w|| 가 작게 만들려 하고, 반면에 두번째 텀은 다음과 같은 설명된 방식에서의 bad 포인트 들에 대한 패널티를 나타냅니다.  
n 개의 훈량 쌍이 있다고 가정해 봅시다. 그리고 d 개의 차원으로, 하나의 데이터 $x_i$ 는 $x_i = [x_{i1},x_{i2},\dots,x_{id}]$  와 같습니다.  
w 도 $w=[w_1,w_2,\dots,w_d]$ 입니다. 그리고 이 두개의 서메이션은 벡터들의 dot product 를 의미합니다.  

상수 C 는 regularization parameter 라고 부릅니다. 이것은 오분류가 얼마나 중요한지를 반영합니다.  
만약 잘못 분류되는 것을 원치 않는다면, C 의 값을 크게 선택하면되지만, 마진은 좁게 될것입니다.  
잘못분류되는 것이 어느 정도 괜찮다면, C 를 작게 선택하고 마진을 크게 설정할 수 있습니다.

식 12.4 에서 패널티 함수는 두번째 항입니다.

<img src="12.Large-Scale Machine Learning.assets/image-20200727232143452.png" alt="image-20200727232143452" style="zoom:50%;" />

L 은 hinge function 입니다. 이는 아래 그림과 같고, 이것의 값을 hinge loss 라고 합니다.

<img src="12.Large-Scale Machine Learning.assets/image-20200727232238858.png" alt="image-20200727232238858" style="zoom:50%;" />

z를 위와 같이 두었을 때, $z_i$ 가 1 이상이면 L의 값은 0 이 됩니다.  
$z_i$ 값이 더 작으면 L 의 값은 z가 감소함에 따라 증가합니다.

우리는 $L(x_i,y_i) $ 의 각 $w_j$ 에 관한 미분을 취해야합니다.  
그리고 hinge function의 미분은 비연속입니다.  
$z_i$가 1보다 작으면 $-y_ix_{ij}$ 이고,  
 $z_i$가 1보다 크면 0 이됩니다.  

그래서 $y_i =+1 $ 일 때는

<img src="12.Large-Scale Machine Learning.assets/image-20200727232812605.png" alt="image-20200727232812605" style="zoom:50%;" />

이렇게 되고, $y_i = -1$ 일 때는  

<img src="12.Large-Scale Machine Learning.assets/image-20200727232846294.png" alt="image-20200727232846294" style="zoom:50%;" />

두 경우를 합하면 다음과 같은 식이 나옵니다.

<img src="12.Large-Scale Machine Learning.assets/image-20200727232924685.png" alt="image-20200727232924685" style="zoom:50%;" />



### 12.3.4 SVM Solutions by Gradient Descent

위의 식을 풀기 위해서는 quadratic 을 사용해야 합니다.  
대규모의 데이터에서는 gradient descent 가 좋습니다.  
우리는 데이터가 메모리보다 디스크에 있도록 할 수 있습니다.   
경사하강법을 구현하기 위해, 우리는 벡터 w 의 성분 wj 각각 과 b 에 관한 식의 미분을 계산합니다.  
우리는 f(w,b) 를 최소화 하기 원하기 때문에, b와 성분 wj를 그래디언트 방향과 반대 방향으로 이동합니다.  
각 구성 요소를 이동하는 양은 해당 구성 요소에 대한 미분에 비례합니다.

첫 번째 단계는 섹션 12.2.4의 트릭을 사용하여 b를 가중치 벡터 w의 일부로 만드는 것입니다.   
b는 실제로 내적 w.x에 대한 임계 값의 음수이므로,  
(d+1) 번째 성분 b를 w에 추가하고  
값이 +1 인 추가 성분을 훈련 세트의 모든 특징 벡터에 추가 할 수 있습니다.

각 라운드에서 w를 이동시키는 그래디언트의 비율이되도록 상수 η을 선택해야합니다. 

<img src="12.Large-Scale Machine Learning.assets/image-20200727234943668.png" alt="image-20200727234943668" style="zoom:50%;" />

첫번째 텀의 wi 에 대한 편미분 은 쉽게 할 수 있습니다.  
그러나 두번째 텀 hinge function 은 좀 어렵습니다. if 표현을 사용해서 다음과 같이 나타냅니다.

<img src="12.Large-Scale Machine Learning.assets/image-20200727235123592.png" alt="image-20200727235123592" style="zoom:50%;" />

이 공식은 가중치 w1, w2. . . , wd. 뿐만 아니라 b 인 $w_{d+1}$을 포함하여 w의 각 구성 요소에 대한 부분 미분을 제공합니다 if-then 조건에서 동등한 wd 1 대신 b를 계속 사용하여 원하는 하이퍼 플레인이 설명 된 형식을 상기시킵니다.

경사하강 알고리즘을 수행하기 위해서 다음을 선택해야 합니다.

1. 파라미처 C 와 $\eta$
2. w 의 초기 값, (d+1)번째인 b 까지 포함해서

그러고 반복합니다.

1. $w_j$ 에 대해 f(w,b) 의 편미분을 계산합니다.
2. $w_j$ 에 $\eta \frac{\partial f}{\partial w_j}$ 를 빼서 w 닶을 조절합니다.

<img src="12.Large-Scale Machine Learning.assets/image-20200728145050995.png" alt="image-20200728145050995" style="zoom:50%;" />

이와 같이 주어졌을 때

트레이닝 셋은 아래와 같고

<img src="12.Large-Scale Machine Learning.assets/image-20200728145115848.png" alt="image-20200728145115848" style="zoom:50%;" />

<img src="12.Large-Scale Machine Learning.assets/image-20200728145409928.png" alt="image-20200728145409928" style="zoom:50%;" />

bad 포인트 항에 대해서 위와 같이 계산할 수 있습니다.

6번의 학습을 통해 다음과 같은 결과를 얻을 수 있습니다.

<img src="12.Large-Scale Machine Learning.assets/image-20200728145546816.png" alt="image-20200728145546816" style="zoom:50%;" />



### 12.3.5 Stochastic Gradient Descent

경사하강 알고리즘은 종종 batch graedient descent 라고도 불립니다.  
왜냐면 각 라운드에, 모든 훈련 데이터를 하나의 batch로 보기 때문입니다.  
작은 데이터셋에서 효과적인 반면에, 큰 데이터셋에서는 실행하는데 많은 시간이 걸릴 수 있습니다.  
이 상황에서는 모든 훈렷 데이터에 대해 수행하고, 수렴할 때 까지 계속 반복해야합니다.  

확률 적 그라디언트 디센트 (stochastic gradient descent)라고하는 대안은 한 번에 하나의 훈련 예 또는 몇 가지 훈련 예를 고려하고 작은 훈련 세트만으로 지시 된 방향으로 오차 함수 (SVM 예에서 w)의 현재 추정치를 조정합니다. 다른 훈련 세트를 사용한 추가적인 라운드가 가능하고, 이것은 무작위나 전략에 따라 선택될 수 있습니다.  
트레이닝 세트의 일부 멤버가 확률 적 그라디언트 디센트 알고리즘에 사용되지 않는 것이 일반적입니다.

미니 배치 그라디언트 디센트라고하는 배치 및 확률 적 그라디언트 디센트 사이에는 절충안이 있습니다. 미니 배치 버전에서는 전체 훈련 세트를 선택된 크기 (예 : 1000 개 훈련 예)의 "미니 배치"로 분할합니다. 우리는 한 번에 하나의 미니 배치를 작업하면서 방정식 12.4를 사용하여 변경 사항을 w로 계산하지만 선택한 교육 예제에 대해서만 합산합니다.



### 12.3.6 Parallel Implementation of SVM

SGD 가 내제적으로 serial 이라는 것에 주목해봅시다.  w와 b 는 모든 트래이닝 데이터들을 고려해서 바뀝니다.   
반면 batch GD 는 대부분 쉽게 병렬화됩니다. 왜냐하면 각 훈련 데이터들은 동일한 상태에서 시작해서 마지막라운드에 합쳐집니다.  

그래서 우리는 는  SVM 을 GD 를 사용하여 병렬화 할 수 있습니다.(12.2.8과 같이)  
w,b 를 미니배치로 나누고, 미니배치 당 하나의 task 를 만듭니다.  
각 미니배치에 12.4 번식을 적용하고  w,b에 대한 변화는 하나의 병렬 라운드마다 합쳐집니다.  
모든 변화를 합치면서 state가 계산되고, 이 과정은 새로은 상태가 모든 테스크에게 분배되면서 반복할 수 있습니다. 



DT 는 공간을 모델링하는 것이 아니라 의사 결정 경계를 세웁니다.  
다른 (훈련) 데이터의 attrebute 의 평균을 사용해 



### 12.5 Decision Trees

DT 는 인풋이 속한 클래스를 생성하기 위해 피쳐 벡터의 속성을 사용하는 분기 프로그램입니다.  
우리는 트리의 형태에서 만들어진 결정(decision) 을 보여줍니다.  
이번 세션에서는 우리는 훈련 데이러를 정확하게 분류하는 트리를 어떻게 설계하는지 다룹니다.  
의사결정나무에서 각 nonleaf 노드는 입력에서의 test를 표현합니다.   
그것의 자식노드들은 테스트들(ellipses) 이나 leaf(output, rectangle)들 입니다.  
테스트 노드의 자식 노드들은 테스트의 결과에 의해 라벨됩니다.  
일반적으로 오직 2개의 아웃컴이 있습니다. - T/F  그러나 테스트의 아웃컴이 숫자는 어느 것이든 될 수 있습니다.  
또한 이 세션에서는, 우리는 검사하빈다. 병렬적으로 사용하는 검사 방법을, 가장 효과적인 트리를 찾기 위해.  
오버피팅이 일반적인 문제일 때, 우리는 또한 어떻게 노드들의 제거하여 트리를 단순화 하는지에 대해 다룹니다. 많은 정확도를 줄이지 않고 오버피팅을 줄이는

### 12.5.1 Using a Decision Tree

트레이닝 데이터와 그것으로 만들어진 트리의 예시로 시작해봅시다.  
아래 그림은 데이터의 정보를 담고 있습니다. 주어진 정보를 사용해 output- favortie sport 를 클래스로 예측합니다.

<img src="12.Large-Scale Machine Learning.assets/image-20200729100825073.png" alt="image-20200729100825073" style="zoom:50%;" />

<img src="12.Large-Scale Machine Learning.assets/image-20200729101506164.png" alt="image-20200729101506164" style="zoom:50%;" />

국가마다 좋아하는 스포츠를 예측하기 원합니다   
국가들의 인구과 대륙이 주어졌을 때, 우리는 루트부터 시작해서 루트에 test 를 적용합니다.  
우리는 테스트의 결과가 t 면 왼쪽, f 면 오른쪽으로 가게 합니다.  
우리나 nonleaef 노드에 있을 때는 테스트를 적용해 노드가 왼쪽 혹은 오른쪽으로 가게 합니다. (만족하는 지에 따라)  
우리가 만약 리프에 도달했을 때, 리프의 클래스를 출력합니다.

너는 이 트리를 사용해 정확하게 12개의 국가들이 분류되는 것을 체크하길 원합니다.   
예를 들어 스페인에서 이는 유럽 대륙에 속합니다. 그래서 루트를 만족하고 왼쪽으로 갑니다.  
인구가 60 70 사이인지 묻고, 아니므로 오른쪽으로 갑니다. 그래서 soccer 가 나오고 이것은 spain 이 좋아하는 스포츠가 맞습니다.  

그러나 12.25에 있는 많은 나라들이 없고 트리는 그것들에 대해 잘 작동하지는 않습니다.  
예를들어 파키스탄의 경우 asia / 182백만입니다.  
루트 부터 시작해서 오른쪽 으로 가고 왼쪽으로 가서 하키가 나옵니다.

이러한 문제 상황을 오버피팅이라고 합니다.  
루트에서의 테스트는 괜찮아 보입니다.  축구는 남아메리카와 유럽에서 가장 인기가 있습니다.  
그러나 인구에 대한 것은 유용하지 않습니다.  이것은 국가의 크기가 그들이 어떻 스포츠를 좋아하는지와 관련이 없기 떄문입니다.  
우리는 이 예에서 인구를 사용하여 그림 12.25에서 나무의 노드에 도달하는 소수의 국가를 구별했습니다. 그러나 뿌리와는 달리, 2 단계 시험은 실제로 보이지 않는 더 큰 국가에 적용되는 것을 말하지 않습니다.

### 12.5.2 Impurity Measure

의사결정나무를 설계하기 위해서, 우리는 좋은 테스트를 선택하는 것이 필요합니다.  
우리는 가능한 적을 레벨을 사용하고자 합니다. 그래서 새로운 데이터를 빠르게 분류할 수 있습니다.  그리고 우리는 오버피팅을 피할 수 있습니다.  
이상적으로 우리는 특정 노드에 가는 모든 입력들이 같은 클래스를 가지글ㄹ 원합니다. 왜냐하면 그래야 우리가 그 노드를 리프로 만들고 그 노드로 가는 훈련 데이터들을 정확하게 분류할 수 있습니다.  

우리는 impurity 의 개념을 가지고 노드에 대한 속성을 수식화 할 수 있습니다.  
우리가 사용할 수 잇는 많은 impurity 측정법이 있습니다.  
그것들 모두 속성을 갖고 있습니다. - 단일 클래스를 가진 예제들을 훈련시킨 경우 도달한 노드의 속성은 0 입니다.  
일반적인 impurity 측정하는 3가지 방법이 있습니다.  
각각은 n 개의 클래스가있는 훈련 예에 의해 도달 된 노드에 적용되며, pi는 i 번째 클래스에 속하는 훈련 예의 일부입니다.

1. Accuracy : 정확하게 분류된 입력들이 도달하는 비율  
   $1 - \max (p_1,p_2,\dots,p_n)$
2. GINI Impurity : $1 - \sum_{i=1}^n(p_i)^2$
3. Entropy : $\sum_{i=1}^n p_i \log_2 (1/p_i)$

#### example 12.15 

위의 예시에서 4개의 클래스가 있습니다.  
Soccer 의 p는 5/12 이고, baseball,Hocket 는 1/6 이고, Cricket 은 1/4 입니다.  
루트의 accuracy 는 1-5/12 = 0.583,   
루트의 GINI 는 <img src="12.Large-Scale Machine Learning.assets/image-20200729104936312.png" alt="image-20200729104936312" style="zoom:33%;"/>=0.715  
루트의 entory 는<img src="12.Large-Scale Machine Learning.assets/image-20200729105020717.png" alt="image-20200729105020717" style="zoom:50%;" />

이러한 impurity 측정 값이 다소 다른 값을 갖는다는 사실은 중요하지 않습니다. 이 측정 값은 가능한 범위가 다릅니다.  

루트위 왼쪽 자식 노드는 덜 impure 합니다.  
SA와 유럽의 6개의 국가들이 도달합니다. 그리고 그 중 5국가는 soccer, 나머지는 cricket 을 좋아합니다.  
이 노드의 impurity 는   
1-5/6=.167  / 1 - (1/6)^2  - (5/6)^2 = .278 / .. = .643

### 12.5.3 Designing a Decision-Tree Node

노드 설계의 목표는 가중 평균 impurity 이 가능한 한 작은 자식노드 를 생산하는 것인데,  
여기서 자식노드의 가중은 노드에 도달하는 훈련 예의 수에 비례합니다.  
원칙적으로, 노드에서의 test 는 입력의 어떠한 함수가 될 수 있습니다.  
가능성들의 이 집합은 본질적으로 무한함에 다라, 우리는 가능한 테스트들을 바이너리 결정으로 제한해야합니다. 다음과 같은 두가지 요소들을 기반으로

1. 상수를 가진 입력 벡터의 하나의 수치적 feature  비교
2. 입력 벡터의 하나의 범주적 feature가 가능한 값들의 집합에 있는지 테스트

#### example 12.16  

12.26의 그림에서 루트의 자식에서 테스트는 조건 1을 만족하지 않습니다. (수치에 대한)  
예를들어, 루트의 오른쪽 자식은 두 비교의 논리적 and 입니다. (인구>55 , 인구<200)  
그러나 우리는 이러한 조건들 모두 사용할 수 있습니다. 만약 우리가 하나의 노드를 두개로 대체한다면.  
루트는 테스트입니다. SA 와 EUR 라는 대륙의 **집합**에 포함된 값을 만족하는지, 이것은 조건 2를 만족합니다.

훈련 앰플의 부분집합에 도달하는 노드가 주어졌다고 가정해봅시다.  
만약 이 노드가 pure 하면 (훈련 앰플들이 동일한 아웃풋을 갖는다.) 그러면 우리는 그 노드를 아웃풋과 같은 값을 갖는 leaf 로 만들면 됩니다.  
만약 impurity 가 0보다 크다면 impurity 가 가장 많이 줄어드는 test 를 찾아야 합니다.  
그러한 테스트를 선택할때, 입력의 어떤 feature 를 선택해도 상관없습니다.  
만약 numerical 피쳐를 선택하면, 우리는 훈련 앰플을 두개의 집합으로 나누는 어떠한 상수를 선택할 수 있습니다. 하나는 왼쪽 자식으로 가고 하나는 오른쪽으로 가는.  
이와같이, 범주형 피쳐를 선택한다면, 우리는 멤버쉽 테스트를 위한 값들의 어떠한 집합도 선택할 수 있습니다. 이러한 경우들을 순서대로 봅시다.

### 12.5.4 Selecting a Test Using a Numerical Feature

수치적 피쳐 A 에 기반하여 집합을 나눈다고 가정해봅시다.  
수행하는 예시는, A 는 population 만 될 수 있습니다. (인구와 대륙 중)  
최고의 breakpoint 를 선택하기 위해 우리는

1. A의 값들에 따라 훈련 앰플들을 나열합니다. $a_1,a_,\dots,a_n$
2. j =1,2,..n 에 대해, 훈련 앰플의 숫자를 계산합니다. - 각 클래스에 속하는  
   j 번째 예제 이후 클래스의 수는 j-1의 수와 같거나 (이 클래스에없는 j 번째 예인 경우) j의 수보다 하나 더 많기 때문에 이러한 카운트는 증분식으로 수행 할 수 있습니다. 1 (j 번째 예제가이 클래스에있는 경우).
3. 이전단계서 계산한 카운터부터, weighted average impurity 를 계산합니다.앞에서 j 개는 왼쪽 n-j 개는 오른쪽으로 둡니다.  
   impurity 는 카운트 할 때 각 클래스 별로 계산되어야 합니다.  
   impurity 계산은 accuracy, GINI, entropy 로 할 수 있습니다.
4. weighted-average impurity 가 최소가 되는 j 의 값을 선택합니다.  
   그러나 이 단계에서 가능한 모든 j 값을 사용할 수 있는 것은 아닙니다,  $a_j = a_{j+1}$ 도 가능하기 때문입니다.  
   우리는 $a_j <a_{j+1}$ 와 같은 값인 j 를 선택하는 것을 제약하기 위해서 $A <(a_j +a_{j+1})/2$ 을 비교로 사용할 수 있습니다.

#### example 12.17 

우리가 루트 비교를 사용한다고 가졍해 봅시다.   
이것은 유럽과 남아메리카 의 6개의 국가를 왼쪽으로 보내고 나머지 6개를 오른쪽으로 보냅니다.  
우리는 이제 왼쪽 자식으로 도달하는 6개의 나라들을 나눕니다. - 루트의 왼쪽 자식의 두 자식들의 weighted impurity 가 가장 작도록 하여  
우리는 대륙이나 인구 둘다 분할에 사용할 수 있습니다. 그리고 우리는 반드시 둘다 고려해봐야합니다. 최소로 나누는 것을  
12.27 그림에서 우리는 인구 순으로 루트 왼쪽 자식에 도달한 6개의 국가들을 볼 수 있습니다.

<img src="12.Large-Scale Machine Learning.assets/image-20200729152805473.png" alt="image-20200729152805473" style="zoom:50%;" />

6개의 칼럼은 다음과 같이 계산됩니다.  
첫번째 Sp 는 좋아하는 운동입니다. S - soccer, C - Csricket 입니다.  
ns 와 nc 는 누적 카운트입니다.   german 의 ns=4 는Arg 부터 5개의 row 중에 4개가 S 인 것입니다.  
ps<- 와 pc<- 는 각각 s와 c 의 해당행까지의 분수입니다.   
ps> pc> 는 같은 분수이지만 문제의 행 아래에있는 행에 대한 분수입니다.(자기 포함 안함)  

IM<- 와 IM> 를 봅시다.  이것은 왼쪽과 오른쪽 자식의 불순도 입니다. GINI 를 사용하였으므로  
$IM\leq \ = 1- ( (p_s\leq)^2 + (p_c\leq)^2)  $ 이고,  $IM> \ = 1- ( (p_s>)^2 + (p_c>)^2)  $ 

마지막으로  weighted GINI impurity 를 구합니다. 각 자식에 도달하는 나라의 수로 웨이트를 줍니다.  
스페인의 경우 (2/6) 0 + (4/6) (3/8) = 1/4 로 구할 수 있습니다.   
위에서는 2/9 가 가장 작은 impurity 로 Italy 로 분할할 수 있습니다. - population <60  
기술적으로, 우리는 또한 루트의 왼쪽 자식에 대한 테스트에는 인구가 전혀 포함되지 않고 대륙에 대한 테스트라는 것을 고려해야합니다. 그러나 유럽을 남미에서 분리하여 더 나은 성과를 거둘 수는 없습니다. 이는 가중 GINI 불순물 1/4을 제공합니다.

### 12.5.5 Selecting a Test Using Categorical Feature

우리는 이제 어



<img src="12.Large-Scale Machine Learning.assets/image-20200729224637364.png" alt="image-20200729224637364" style="zoom:50%;" />

<img src="12.Large-Scale Machine Learning.assets/image-20200730110457373.png" alt="image-20200730110457373" style="zoom:50%;" />